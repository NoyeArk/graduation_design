{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from data import BPRSampleGenerator, SeqBPRDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model import SeqLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_path': '/graduation_design/',\n",
       " 'topk': 10,\n",
       " 'data': {'train_valid_split': 0.95,\n",
       "  'maxlen': 30,\n",
       "  'name': 'ml-1m',\n",
       "  'sep': '::',\n",
       "  'item_path': '/graduation_design/data/ml-1m/movies.dat',\n",
       "  'item_emb_path': '/graduation_design/data/ml-1m/item_embeddings.npy',\n",
       "  'path': '/graduation_design/data/ml-1m/ratings.dat',\n",
       "  'num_negatives': 1,\n",
       "  'user_threshold': 10,\n",
       "  'item_threshold': 10,\n",
       "  'rating_threshold': 2,\n",
       "  'base_model': ['acf', 'fdsa', 'harnn', 'caser', 'pfmc', 'sasrec', 'anam'],\n",
       "  'base_model_path': '/graduation_design/base_model_results/ml-1m'},\n",
       " 'model': {'lr': 0.001,\n",
       "  'type': 'SASEM',\n",
       "  'lamda': 1e-05,\n",
       "  'hidden_dim': 32,\n",
       "  'device': 'cuda:0',\n",
       "  'optimizer': 'AdamOptimizer',\n",
       "  'tradeoff': 2,\n",
       "  'div_module': 'cov',\n",
       "  'pretrain_llm': 'bert-base-uncased'},\n",
       " 'epoch': 1,\n",
       " 'batch_size': 512}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/graduation_design/bpr/config/bpr.yaml\", 'r', encoding='utf-8') as f:\n",
    "    args = yaml.unsafe_load(f)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/graduation_design/base_model_results/ml-1m/acf.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acf \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_model_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/acf.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/graduation_design/base_model_results/ml-1m/acf.npy'"
     ]
    }
   ],
   "source": [
    "acf = np.load(args['data']['base_model_path'] + f\"/acf.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 交互索引范围: 最小值 = 0, 最大值 = 834448\n",
      ">>>> 数据加载完成: 834449 条交互, 6033 个用户, 3123 个物品\n",
      ">>>> 基模型的预测结果加载完成: (834449, 7, 102)\n",
      ">>>> 构建了 6033 个用户的历史交互序列\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>> 生成序列样本: 100%|██████████| 6033/6033 [00:02<00:00, 2081.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 生成了 828416 个序列感知样本\n"
     ]
    }
   ],
   "source": [
    "# 创建数据生成器\n",
    "generator = BPRSampleGenerator(args['data'])\n",
    "seq_samples = generator.generate_seq_samples(\n",
    "    seq_len=args['data']['maxlen'],\n",
    "    num_negatives=args['data']['num_negatives']\n",
    ")\n",
    "\n",
    "# 创建数据集\n",
    "dataset = SeqBPRDataset(seq_samples, args['model']['device'])\n",
    "train_size = int(args['data']['train_valid_split'] * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算NDCG@10...: 100%|██████████| 41421/41421 [00:15<00:00, 2673.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.219812818637405"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = np.load(args['data']['base_model_path'] + f\"/sasrec.npy\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    ndcg_scores = []\n",
    "    for batch in tqdm(test_loader, desc=\"计算NDCG@10...\"):\n",
    "        users, user_seq, pos_items, neg_items, base_model_preds = batch\n",
    "        \n",
    "        # 遍历batch中的每个样本\n",
    "        for i in range(len(users)):\n",
    "            # 获取交互索引\n",
    "            user_id = users[i].item()\n",
    "            pos_item_id = pos_items[i].item()\n",
    "            interaction_idx = generator.get_interaction_index(user_id, pos_item_id)\n",
    "            if interaction_idx is None:\n",
    "                continue\n",
    "\n",
    "            # 获取模型推荐的top物品列表\n",
    "            top_items = model[interaction_idx][2:2+args['topk']]\n",
    "\n",
    "            # 获取用户的实际交互物品\n",
    "            true_items = generator.user_interacted_items[user_id]\n",
    "\n",
    "            # 计算DCG\n",
    "            dcg = 0\n",
    "            for j, item_idx in enumerate(top_items):\n",
    "                if item_idx in true_items:\n",
    "                    dcg += 1 / np.log2(j + 2)\n",
    "            \n",
    "            # 计算IDCG\n",
    "            idcg = 0\n",
    "            for j in range(min(len(true_items), args['topk'])):\n",
    "                idcg += 1 / np.log2(j + 2)\n",
    "\n",
    "            # 计算NDCG\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            ndcg_scores.append(ndcg)\n",
    "\n",
    "np.mean(ndcg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成模型预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 加载预计算的物品嵌入...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SeqLearn(\n",
       "  (cem): ContentExtractionModule(\n",
       "    (llm): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSdpaSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): RobertaPooler(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (pooling): AdaptiveAvgPool2d(output_size=(1, 1024))\n",
       "  )\n",
       "  (user_embeddings): Embedding(6033, 32)\n",
       "  (item_tower): ItemTower(\n",
       "    (item_transform): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    )\n",
       "    (preference_alignment): PreferenceAlignmentModule(\n",
       "      (content_adaptor): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (position_embeddings): Embedding(30, 32)\n",
       "      (transformer_layers): ModuleList(\n",
       "        (0-1): 2 x TransformerBlock(\n",
       "          (attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (online_projection): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (llm_projection): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (gru): GRU(32, 32, batch_first=True)\n",
       "  (attention_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (self_attention_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (self_attention_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (self_attention_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (self_attention_output): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (layer_norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (layer_norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (augru): GRU(32, 32, batch_first=True)\n",
       "  (score_layer): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False, drop_last=True)\n",
    "\n",
    "model = SeqLearn(args['model'], args['data'], 6033, generator.n_item)\n",
    "# 加载checkpoint\n",
    "ckpt = torch.load(f\"/root/autodl-tmp/score_ckpt/score_epoch1_batch900.pth\")\n",
    "\n",
    "# 过滤掉不需要加载的层\n",
    "filtered_ckpt = {k: v for k, v in ckpt.items() if not k.startswith('item_tower.cex')}\n",
    "\n",
    "# 加载过滤后的权重\n",
    "model.load_state_dict(filtered_ckpt, strict=False)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户张量形状: torch.Size([512])\n",
      "用户序列张量形状: torch.Size([512, 30])\n",
      "物品张量形状: torch.Size([512])\n",
      "分数张量形状: torch.Size([512])\n",
      "基础模型预测张量形状: torch.Size([512, 7, 100])\n"
     ]
    }
   ],
   "source": [
    "# 获取一个批次的数据\n",
    "batch = next(iter(test_loader))\n",
    "users, user_seq, items, scores, base_model_preds = batch\n",
    "\n",
    "print(\"用户张量形状:\", users.shape)\n",
    "print(\"用户序列张量形状:\", user_seq.shape) \n",
    "print(\"物品张量形状:\", items.shape)\n",
    "print(\"分数张量形状:\", scores.shape)\n",
    "print(\"基础模型预测张量形状:\", base_model_preds.shape if base_model_preds is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512]) torch.Size([512, 30]) torch.Size([512]) torch.Size([512, 7, 100])\n",
      "torch.Size([512]) torch.Size([512, 30]) torch.Size([512]) torch.Size([512, 7, 100])\n",
      "torch.Size([512]) torch.Size([512, 30]) torch.Size([512]) torch.Size([512, 7, 100])\n",
      "torch.Size([512]) torch.Size([512, 30]) torch.Size([512]) torch.Size([512, 7, 100])\n",
      "torch.Size([512]) torch.Size([512, 30]) torch.Size([512]) torch.Size([512, 7, 100])\n",
      "torch.Size([512]) torch.Size([512, 30]) torch.Size([512]) torch.Size([512, 7, 100])\n",
      "torch.Size([51]) torch.Size([51, 30]) torch.Size([51]) torch.Size([51, 7, 100])\n"
     ]
    }
   ],
   "source": [
    "user_idx = 0\n",
    "user = users[user_idx:user_idx+1]\n",
    "user_seq_i = user_seq[user_idx:user_idx+1]\n",
    "base_model_preds_i = base_model_preds[user_idx:user_idx+1] if base_model_preds is not None else None\n",
    "\n",
    "# 获取所有物品ID\n",
    "all_item_ids = torch.arange(generator.n_item, device=args['model']['device'])\n",
    "\n",
    "all_scores = []\n",
    "\n",
    "for i in range(0, len(all_item_ids), args['batch_size']):\n",
    "    batch_items = all_item_ids[i:i + args['batch_size']]\n",
    "    cnt = len(batch_items)\n",
    "\n",
    "    user_repeated = user.repeat(cnt)\n",
    "    user_seq_repeated = user_seq_i.repeat(cnt, 1)\n",
    "    base_model_preds_repeated = base_model_preds_i.repeat(cnt, 1, 1)\n",
    "    # 预测分数\n",
    "    print(user_repeated.shape, user_seq_repeated.shape, batch_items.shape, base_model_preds_repeated.shape)\n",
    "    batch_scores = model(user_repeated, user_seq_repeated, batch_items, base_model_preds_repeated)\n",
    "    all_scores.append(batch_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户779的预测分数范围: 3.9388139247894287 ~ 3.9545481204986572\n"
     ]
    }
   ],
   "source": [
    "user_scores = torch.cat(all_scores, dim=0).squeeze()\n",
    "print(f\"用户{user.item()}的预测分数范围: {user_scores.min().item()} ~ {user_scores.max().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"计算测试集NDCG\"):\n",
    "        users, user_seq, items, scores, base_model_preds = batch\n",
    "\n",
    "        # 对每个用户计算NDCG\n",
    "        for user_idx in range(len(users)):\n",
    "            # 获取当前用户\n",
    "            user = users[user_idx:user_idx+1]\n",
    "            user_seq_i = user_seq[user_idx:user_idx+1]\n",
    "            base_model_preds_i = base_model_preds[user_idx:user_idx+1] if base_model_preds is not None else None\n",
    "\n",
    "            # 获取所有物品ID\n",
    "            all_item_ids = torch.arange(generator.n_item, device=args['model']['device'])\n",
    "\n",
    "            all_scores = []\n",
    "\n",
    "            for i in range(0, len(all_item_ids), args['batch_size']):\n",
    "                batch_items = all_item_ids[i:i + args['batch_size']]\n",
    "                cnt = len(batch_items)\n",
    "\n",
    "                user_repeated = user.repeat(cnt)\n",
    "                user_seq_repeated = user_seq_i.repeat(cnt, 1)\n",
    "                base_model_preds_repeated = base_model_preds_i.repeat(cnt, 1, 1)\n",
    "                # 预测分数\n",
    "                batch_scores = model(user_repeated, user_seq_repeated, batch_items, base_model_preds_repeated)\n",
    "                all_scores.append(batch_scores)\n",
    "\n",
    "            # 合并所有批次的分数\n",
    "            user_scores = torch.cat(all_scores, dim=0).squeeze()\n",
    "            print(user_scores.shape)\n",
    "\n",
    "            # 获取前k个物品\n",
    "            _, indices = torch.topk(user_scores, args['topk'])\n",
    "\n",
    "            # 获取用户的实际交互物品\n",
    "            true_items = generator.user_interacted_items[user.item()]\n",
    "\n",
    "            # 计算DCG\n",
    "            dcg = 0\n",
    "            for i, item_idx in enumerate(indices):\n",
    "                if item_idx.item() in true_items:\n",
    "                    dcg += 1 / np.log2(i + 2)\n",
    "\n",
    "            # 计算IDCG\n",
    "            idcg = 0\n",
    "            for i in range(min(len(true_items), args['topk'])):\n",
    "                idcg += 1 / np.log2(i + 2)\n",
    "\n",
    "            # 计算NDCG\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            ndcg_scores.append(ndcg)\n",
    "\n",
    "np.mean(ndcg_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
