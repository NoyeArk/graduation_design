{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from data import BPRSampleGenerator, SeqBPRDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model import SeqLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_path': '/graduation_design/',\n",
       " 'topk': 10,\n",
       " 'data': {'train_valid_split': 0.95,\n",
       "  'maxlen': 30,\n",
       "  'name': 'ml-1m',\n",
       "  'sep': '::',\n",
       "  'item_path': '/graduation_design/data/ml-1m/movies.dat',\n",
       "  'item_emb_path': '/graduation_design/data/ml-1m/item_embeddings.npy',\n",
       "  'path': '/graduation_design/data/ml-1m/ratings.dat',\n",
       "  'num_negatives': 1,\n",
       "  'user_threshold': 10,\n",
       "  'item_threshold': 10,\n",
       "  'rating_threshold': 2,\n",
       "  'base_model': ['acf', 'fdsa', 'harnn', 'caser', 'pfmc', 'sasrec', 'anam'],\n",
       "  'base_model_path': '/graduation_design/base_model_results/ml-1m'},\n",
       " 'model': {'lr': 0.001,\n",
       "  'type': 'SASEM',\n",
       "  'lamda': 1e-05,\n",
       "  'hidden_dim': 32,\n",
       "  'device': 'cuda:0',\n",
       "  'optimizer': 'AdamOptimizer',\n",
       "  'tradeoff': 2,\n",
       "  'div_module': 'cov',\n",
       "  'pretrain_llm': 'bert-base-uncased'},\n",
       " 'epoch': 1,\n",
       " 'batch_size': 512}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/graduation_design/bpr/config/bpr.yaml\", 'r', encoding='utf-8') as f:\n",
    "    args = yaml.unsafe_load(f)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/graduation_design/base_model_results/ml-1m/acf.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acf \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_model_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/acf.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/graduation_design/base_model_results/ml-1m/acf.npy'"
     ]
    }
   ],
   "source": [
    "acf = np.load(args['data']['base_model_path'] + f\"/acf.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 交互索引范围: 最小值 = 0, 最大值 = 834448\n",
      ">>>> 数据加载完成: 834449 条交互, 6033 个用户, 3123 个物品\n",
      ">>>> 基模型的预测结果加载完成: (834449, 7, 102)\n",
      ">>>> 构建了 6033 个用户的历史交互序列\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>> 生成序列样本: 100%|██████████| 6033/6033 [03:35<00:00, 27.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 生成了 828416 个序列感知BPR样本对\n"
     ]
    }
   ],
   "source": [
    "# 创建数据生成器\n",
    "generator = BPRSampleGenerator(args['data'])\n",
    "seq_samples = generator.generate_seq_samples(\n",
    "    seq_len=args['data']['maxlen'],\n",
    "    num_negatives=args['data']['num_negatives']\n",
    ")\n",
    "\n",
    "# 创建数据集\n",
    "dataset = SeqBPRDataset(seq_samples, args['model']['device'])\n",
    "train_size = int(args['data']['train_valid_split'] * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算NDCG@10...: 100%|██████████| 41421/41421 [00:15<00:00, 2602.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22138667819592134"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = np.load(args['data']['base_model_path'] + f\"/sasrec.npy\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    ndcg_scores = []\n",
    "    for batch in tqdm(test_loader, desc=\"计算NDCG@10...\"):\n",
    "        users, user_seq, pos_items, neg_items, base_model_preds = batch\n",
    "        \n",
    "        # 遍历batch中的每个样本\n",
    "        for i in range(len(users)):\n",
    "            # 获取交互索引\n",
    "            user_id = users[i].item()\n",
    "            pos_item_id = pos_items[i].item()\n",
    "            interaction_idx = generator.get_interaction_index(user_id, pos_item_id)\n",
    "            if interaction_idx is None:\n",
    "                continue\n",
    "\n",
    "            # 获取模型推荐的top物品列表\n",
    "            top_items = model[interaction_idx][2:2+args['topk']]\n",
    "\n",
    "            # 获取用户的实际交互物品\n",
    "            true_items = generator.user_interacted_items[user_id]\n",
    "\n",
    "            # 计算DCG\n",
    "            dcg = 0\n",
    "            for j, item_idx in enumerate(top_items):\n",
    "                if item_idx in true_items:\n",
    "                    dcg += 1 / np.log2(j + 2)\n",
    "            \n",
    "            # 计算IDCG\n",
    "            idcg = 0\n",
    "            for j in range(min(len(true_items), args['topk'])):\n",
    "                idcg += 1 / np.log2(j + 2)\n",
    "\n",
    "            # 计算NDCG\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            ndcg_scores.append(ndcg)\n",
    "\n",
    "np.mean(ndcg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成模型预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 加载预计算的物品嵌入...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算测试集NDCG:   0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_emb time: 8.0108642578125e-05\n",
      "user_interaction time: 1.891808032989502\n",
      "preference time: 0.0018796920776367188\n",
      "base_model_focus_llm time: 0.0003285408020019531\n",
      "each_model_emb time: 6.985664367675781e-05\n",
      "basemodel_emb time: 3.314018249511719e-05\n",
      "wgts time: 5.936622619628906e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算测试集NDCG:   1%|▏         | 1/80 [00:02<03:18,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_emb time: 0.4733603000640869\n",
      "weighted_basemodel_emb time: 0.0003726482391357422\n",
      "weighted_basemodel_emb_expanded time: 4.5299530029296875e-06\n",
      "item_emb_expanded time: 1.3828277587890625e-05\n",
      "concat time: 3.1948089599609375e-05\n",
      "scores time: 0.00019097328186035156\n",
      "user_emb time: 6.008148193359375e-05\n",
      "user_interaction time: 1.8978071212768555\n",
      "preference time: 0.0018105506896972656\n",
      "base_model_focus_llm time: 0.0003025531768798828\n",
      "each_model_emb time: 6.508827209472656e-05\n",
      "basemodel_emb time: 3.314018249511719e-05\n",
      "wgts time: 6.198883056640625e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算测试集NDCG:   2%|▎         | 2/80 [00:05<03:16,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_emb time: 0.47461700439453125\n",
      "weighted_basemodel_emb time: 0.0004582405090332031\n",
      "weighted_basemodel_emb_expanded time: 4.291534423828125e-06\n",
      "item_emb_expanded time: 1.5974044799804688e-05\n",
      "concat time: 3.123283386230469e-05\n",
      "scores time: 0.00021028518676757812\n",
      "user_emb time: 6.413459777832031e-05\n",
      "user_interaction time: 1.892831563949585\n",
      "preference time: 0.0018486976623535156\n",
      "base_model_focus_llm time: 0.0003120899200439453\n",
      "each_model_emb time: 6.508827209472656e-05\n",
      "basemodel_emb time: 3.147125244140625e-05\n",
      "wgts time: 5.984306335449219e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算测试集NDCG:   4%|▍         | 3/80 [00:07<03:13,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_emb time: 0.47347211837768555\n",
      "weighted_basemodel_emb time: 0.0003848075866699219\n",
      "weighted_basemodel_emb_expanded time: 4.0531158447265625e-06\n",
      "item_emb_expanded time: 1.33514404296875e-05\n",
      "concat time: 3.075599670410156e-05\n",
      "scores time: 0.00018095970153808594\n",
      "user_emb time: 7.534027099609375e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算测试集NDCG:   4%|▍         | 3/80 [00:08<03:39,  2.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 获取所有物品的预测分数\u001b[39;00m\n\u001b[1;32m     20\u001b[0m all_items \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(generator\u001b[38;5;241m.\u001b[39mitem_to_id))\u001b[38;5;241m.\u001b[39mto(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43musers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m _, indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(all_scores, args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopk\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(users)):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# 获取用户的实际交互物品\u001b[39;00m\n",
      "File \u001b[0;32m/graduation_design/bpr/model.py:284\u001b[0m, in \u001b[0;36mSeqLearn.predict\u001b[0;34m(self, users, user_seq, items, base_model_preds)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_emb time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m st \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 284\u001b[0m user_interaction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_seq\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# bc, seq_len, hidden_dim\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_interaction time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m st \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/graduation_design/bpr/module/learn.py:381\u001b[0m, in \u001b[0;36mItemTower.forward\u001b[0;34m(self, item_id)\u001b[0m\n\u001b[1;32m    376\u001b[0m         item_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(item_embeddings)\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m item_embeddings\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUserTower\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m    382\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m    用户塔\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03m    结合内容提取模块和偏好对齐模块，生成用户嵌入\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, pretrained_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    387\u001b[0m                  max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_transformer_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, num_attention_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[1;32m    388\u001b[0m                  intermediate_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3072\u001b[39m, max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False, drop_last=True)\n",
    "\n",
    "model = SeqLearn(args['model'], args['data'], 6033, generator.n_item)\n",
    "# 加载checkpoint\n",
    "ckpt = torch.load(f\"/graduation_design/bpr/ckpt/bpr_epoch1_batch60.pth\")\n",
    "\n",
    "# 过滤掉不需要加载的层\n",
    "filtered_ckpt = {k: v for k, v in ckpt.items() if not k.startswith('item_tower.cex')}\n",
    "\n",
    "# 加载过滤后的权重\n",
    "model.load_state_dict(filtered_ckpt, strict=False)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ndcg_scores = []\n",
    "    for batch in tqdm(test_loader, desc=\"计算测试集NDCG\"):\n",
    "        users, user_seq, pos_items, neg_items, base_model_preds = batch\n",
    "\n",
    "        # 获取所有物品的预测分数\n",
    "        all_items = torch.arange(len(generator.item_to_id)).to(args['model']['device'])\n",
    "        all_scores = model.predict(users, user_seq, all_items, base_model_preds)\n",
    "\n",
    "        _, indices = torch.topk(all_scores, args['topk'])\n",
    "\n",
    "        for user_idx in range(len(users)):\n",
    "            # 获取用户的实际交互物品\n",
    "            true_items = generator.user_interacted_items[users[user_idx].item()]\n",
    "\n",
    "            # 计算DCG\n",
    "            dcg = 0\n",
    "            for i, item_idx in enumerate(indices[user_idx]):\n",
    "                if item_idx.item() in true_items:\n",
    "                    dcg += 1 / np.log2(i + 2)\n",
    "\n",
    "            # 计算IDCG\n",
    "            idcg = 0\n",
    "            for i in range(min(len(true_items), args['topk'])):\n",
    "                idcg += 1 / np.log2(i + 2)\n",
    "\n",
    "            # 计算NDCG\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            ndcg_scores.append(ndcg)\n",
    "\n",
    "np.mean(ndcg_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
