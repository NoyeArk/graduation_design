{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model.sem import Sem\n",
    "from model.ensrec import EnsRec\n",
    "from torch.utils.data import DataLoader\n",
    "from data import Data, SeqBPRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 数据加载完成: 2536880 条交互, 1411 个用户, 3325 个物品\n"
     ]
    }
   ],
   "source": [
    "with open(\"config/bert_config.yaml\", 'r', encoding='utf-8') as f:\n",
    "    args = yaml.unsafe_load(f)\n",
    "data = Data(args['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'datasets/{args[\"data\"][\"name\"]}/train_samples.npy', data.train_samples)\n",
    "np.save(f'datasets/{args[\"data\"][\"name\"]}/test_samples.npy', data.test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.load(f'datasets/{args[\"data\"][\"name\"]}/train_samples.npy', allow_pickle=True)\n",
    "test_samples = np.load(f'datasets/{args[\"data\"][\"name\"]}/test_samples.npy', allow_pickle=True)\n",
    "train_dataset = SeqBPRDataset(train_samples, args['data']['device'])\n",
    "test_dataset = SeqBPRDataset(test_samples, args['data']['device'], is_test=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估指标\n",
    "\n",
    "- nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nDCG(rec_items, test_set):\n",
    "    DCG = lambda x: np.sum(x / np.log(np.arange(2, len(x) + 2)))\n",
    "    def get_implict_matrix(rec_items, test_set):\n",
    "        rel_matrix = [[0] * rec_items.shape[1] for _ in range(rec_items.shape[0])]\n",
    "        for user in range(len(test_set)):\n",
    "            for index, item in enumerate(rec_items[user]):\n",
    "                if item in test_set[user]:\n",
    "                    rel_matrix[user][index] = 1\n",
    "        return np.array(rel_matrix)\n",
    "    rel_matrix = get_implict_matrix(rec_items, test_set)\n",
    "    ndcgs = []\n",
    "    for user in range(len(test_set)):\n",
    "        rels = rel_matrix[user]\n",
    "        dcg = DCG(rels)\n",
    "        idcg = DCG(sorted(rels, reverse=True))\n",
    "        ndcg = dcg / idcg if idcg != 0 else 0\n",
    "        ndcgs.append(ndcg)\n",
    "    return ndcgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "- map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map(recommended_items, interacted_items):\n",
    "    interacted_set = set(interacted_items)\n",
    "    \n",
    "    hits, precisions = [], []\n",
    "    relevant_count = 0\n",
    "    for i, item in enumerate(recommended_items):\n",
    "        position = i + 1  # 位置从1开始计数\n",
    "\n",
    "        is_relevant = item in interacted_set\n",
    "        hits.append(1 if is_relevant else 0)\n",
    "        if is_relevant:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / position\n",
    "            precisions.append(precision_at_k)\n",
    "    \n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "    return sum(precisions) / len(precisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(results, relevant_docs):\n",
    "    relevant_set = set(relevant_docs)\n",
    "    rank = 0\n",
    "    for i, doc_id in enumerate(results):\n",
    "        if doc_id in relevant_set:\n",
    "            rank = i + 1  # 排名从1开始\n",
    "            break\n",
    "    if rank > 0:\n",
    "        reciprocal_rank = 1.0 / rank\n",
    "        return reciprocal_rank\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc1(y_prob, y_label):\n",
    "    n = len(y_prob)\n",
    "    pos_prob = []\n",
    "    neg_prob = []\n",
    "    for i in range(n):\n",
    "        if y_label[i]==1:\n",
    "            pos_prob.append(y_prob[i])\n",
    "        elif y_label[i]==0:\n",
    "            neg_prob.append(y_prob[i])\n",
    "    # 正样本预测概率->负样本预测概率的占比\n",
    "    count = 0\n",
    "    for p in pos_prob:\n",
    "        for n in neg_prob:\n",
    "            if p>n:\n",
    "                count += 1\n",
    "            elif p==n:\n",
    "                count += 0.5\n",
    "    return count/(len(pos_prob)*len(neg_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, topk, metric):\n",
    "    with torch.no_grad():\n",
    "        metrics = []\n",
    "        for batch in tqdm(test_loader, desc=\"计算测试集指标\"):\n",
    "            all_scores = model(batch, is_test=True)\n",
    "            scores, indices = torch.topk(all_scores, topk)\n",
    "\n",
    "            for i in range(len(batch['user_id'])):\n",
    "                user_id = batch['user_id'][i].item()\n",
    "                pos_item = batch['pos_item'][i].item()\n",
    "\n",
    "                true_item_ids = data.user_interacted_item_ids[user_id]\n",
    "                true_item_ids = true_item_ids[true_item_ids.index(data.item_to_id[pos_item]) + 1:]\n",
    "\n",
    "                predicted_item_ids = np.array([indices[i].cpu().numpy().tolist()])\n",
    "                if metric == 'map':\n",
    "                    score = map(predicted_item_ids[0], true_item_ids)\n",
    "                elif metric == 'ndcg':\n",
    "                    score = nDCG(np.array(predicted_item_ids), [true_item_ids])\n",
    "                elif metric == 'mrr':\n",
    "                    score = mrr(predicted_item_ids[0], true_item_ids)\n",
    "                metrics.append(score)\n",
    "    return np.mean(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnsRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> 加载预计算的物品嵌入...\n"
     ]
    }
   ],
   "source": [
    "ensrec = EnsRec(args['model'], args['data'], data.n_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsRec(\n",
       "  (dien): DIEN(\n",
       "    (gru_cell): GRUCell(64, 64)\n",
       "    (attention): AttentionLayer(\n",
       "      (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (augru_cell): GRUCell(128, 64)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (user_embeddings): Embedding(6033, 64)\n",
       "  (item_tower): ItemTower(\n",
       "    (item_transform): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (preference_alignment): PreferenceAlignmentModule(\n",
       "      (content_adaptor): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (position_embeddings): Embedding(20, 64)\n",
       "      (transformer_layers): ModuleList(\n",
       "        (0-1): 2 x TransformerBlock(\n",
       "          (attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (online_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (llm_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"D:\\work_space\\epoch102_0.4187.pth\")\n",
    "# filtered_ckpt = {k: v for k, v in ckpt.items() if not k.startswith('item_tower.cex')}\n",
    "ensrec.load_state_dict(ckpt, strict=False)\n",
    "ensrec.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算测试集指标: 100%|██████████| 17/17 [00:13<00:00,  1.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41873072378852494"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False)\n",
    "infer(ensrec, 10, 'ndcg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem = Sem(args['model'], args['data'], data.n_user, 3952)\n",
    "ckpt = torch.load(f\"../bpr/ckpt_sem/sem_epoch3.pth\")\n",
    "sem.load_state_dict(ckpt, strict=False)\n",
    "sem.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(sem, 10, 'mrr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他集成方法\n",
    "\n",
    "### CombSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算测试集指标: 100%|██████████| 25369/25369 [00:45<00:00, 559.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4071636437917846"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "metrics = []\n",
    "for batch in tqdm(test_loader, desc=\"计算测试集指标\"):\n",
    "    all_scores = batch['all_item_scores'].squeeze(0).sum(dim=0, keepdim=True)\n",
    "    scores, indices = torch.topk(all_scores, 10)\n",
    "\n",
    "    for i in range(len(batch['user_id'])):\n",
    "        user_id = batch['user_id'][i].item()\n",
    "        pos_item = batch['pos_item'][i].item()\n",
    "\n",
    "        true_item_ids = data.user_interacted_item_ids[user_id]\n",
    "        true_item_ids = true_item_ids[true_item_ids.index(data.item_to_id[pos_item]) + 1:]\n",
    "\n",
    "        predicted_item_ids = np.array([indices[i].cpu().numpy().tolist()])\n",
    "        score = nDCG(np.array(predicted_item_ids), [true_item_ids])\n",
    "        metrics.append(score)\n",
    "np.mean(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CombMNZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算测试集指标: 100%|██████████| 25369/25369 [00:58<00:00, 436.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41741408094559723"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "metrics = []\n",
    "for batch in tqdm(test_loader, desc=\"计算测试集指标\"):\n",
    "    all_scores = batch['all_item_scores'].squeeze(0).sum(dim=0, keepdim=True)\n",
    "    item_ids_np = batch['base_model_preds'].cpu().numpy().reshape(-1)\n",
    "    item_indices = torch.tensor(np.array([data.item_to_id[id] for id in item_ids_np]), device=\"cuda\")\n",
    "    all_item_counts = torch.bincount(item_indices, minlength=data.n_item)\n",
    "    all_scores = all_scores * all_item_counts.unsqueeze(0)\n",
    "    scores, indices = torch.topk(all_scores, 10)\n",
    "\n",
    "    user_id = batch['user_id'].item()\n",
    "    pos_item = batch['pos_item'].item()\n",
    "\n",
    "    true_item_ids = data.user_interacted_item_ids[user_id]\n",
    "    true_item_ids = true_item_ids[true_item_ids.index(data.item_to_id[pos_item]) + 1:]\n",
    "\n",
    "    predicted_item_ids = np.array([indices[0].cpu().numpy().tolist()])\n",
    "    score = nDCG(np.array(predicted_item_ids), [true_item_ids])\n",
    "    metrics.append(score)\n",
    "np.mean(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ​CombANZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算测试集指标: 100%|██████████| 25369/25369 [00:59<00:00, 424.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4353325855614328"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "metrics = []\n",
    "for batch in tqdm(test_loader, desc=\"计算测试集指标\"):\n",
    "    all_scores = batch['all_item_scores'].squeeze(0).sum(dim=0, keepdim=True)\n",
    "    item_ids_np = batch['base_model_preds'].cpu().numpy().reshape(-1)\n",
    "    item_indices = torch.tensor(np.array([data.item_to_id[id] for id in item_ids_np]), device=\"cuda\")\n",
    "    all_item_counts = torch.bincount(item_indices, minlength=data.n_item)\n",
    "    # 将计数为0的位置替换为无穷大\n",
    "    all_item_counts[all_item_counts == 0] = 1_000_000_000\n",
    "    all_scores = all_scores / all_item_counts.unsqueeze(0)\n",
    "    scores, indices = torch.topk(all_scores, 10)\n",
    "\n",
    "    user_id = batch['user_id'].item()\n",
    "    pos_item = batch['pos_item'].item()\n",
    "\n",
    "    true_item_ids = data.user_interacted_item_ids[user_id]\n",
    "    true_item_ids = true_item_ids[true_item_ids.index(data.item_to_id[pos_item]) + 1:]\n",
    "\n",
    "    predicted_item_ids = np.array([indices[0].cpu().numpy().tolist()])\n",
    "    score = nDCG(np.array(predicted_item_ids), [true_item_ids])\n",
    "    metrics.append(score)\n",
    "np.mean(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m batch\n",
      "File \u001b[1;32mf:\\anaconda\\anaconda3\\envs\\DeltaZero\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mf:\\anaconda\\anaconda3\\envs\\DeltaZero\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mf:\\anaconda\\anaconda3\\envs\\DeltaZero\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mf:\\anaconda\\anaconda3\\envs\\DeltaZero\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Code\\graduation_design\\bpr\\data.py:322\u001b[0m, in \u001b[0;36mSeqBPRDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m    320\u001b[0m     case \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musers[idx], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m--> 322\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_seq\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistories\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_item\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_items[idx], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_item\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneg_items[idx], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_model_preds\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_preds[idx], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    326\u001b[0m     }\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_test:\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28;01mcase\u001b[39;00m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_item_scores\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_item_scores[idx], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "batch = next(iter(test_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算acf测试集指标...: 100%|██████████| 25369/25369 [00:46<00:00, 541.11it/s, ndcg=[0]]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acf: 0.4407644595632349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算fdsa测试集指标...: 100%|██████████| 25369/25369 [00:41<00:00, 618.68it/s, ndcg=[0]]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fdsa: 0.36706860929264074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算harnn测试集指标...: 100%|██████████| 25369/25369 [00:38<00:00, 655.89it/s, ndcg=[0]]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harnn: 0.46209890972906853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算caser测试集指标...: 100%|██████████| 25369/25369 [00:37<00:00, 673.25it/s, ndcg=[0]]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caser: 0.4352232341441272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算pfmc测试集指标...: 100%|██████████| 25369/25369 [00:40<00:00, 628.41it/s, ndcg=[0]]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pfmc: 0.47393773746545487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算sasrec测试集指标...: 100%|██████████| 25369/25369 [00:40<00:00, 622.14it/s, ndcg=[0]]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sasrec: 0.3920804733319215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算anam测试集指标...: 100%|██████████| 25369/25369 [00:39<00:00, 637.46it/s, ndcg=[0]]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anam: 0.4484931343282081\n"
     ]
    }
   ],
   "source": [
    "base_model_results = {}\n",
    "for base_model in args['data']['base_model']:\n",
    "    model = np.load(args['data']['base_model_path'] + f\"/{base_model}.npy\")\n",
    "    ndcg_scores = []\n",
    "    phar = tqdm(test_samples, desc=f\"计算{base_model}测试集指标...\")\n",
    "    for case in phar:\n",
    "        user_id = case['user_id']\n",
    "        pos_item = case['pos_item']\n",
    "\n",
    "        interaction_idx = data.get_interaction_index(user_id, data.item_to_id[pos_item])\n",
    "        assert interaction_idx != -1\n",
    "\n",
    "        predicted_item_ids = model[interaction_idx][2:2+10]\n",
    "        true_item_ids = data.user_interacted_item_ids[user_id]\n",
    "        true_item_ids = true_item_ids[true_item_ids.index(data.item_to_id[pos_item]) + 1:]\n",
    "\n",
    "        predicted_items = np.array([predicted_item_ids])\n",
    "        ndcg = nDCG(np.array(predicted_items), [true_item_ids])\n",
    "        ndcg_scores.append(ndcg)\n",
    "\n",
    "        phar.set_postfix(ndcg=ndcg)\n",
    "    print(f\"{base_model}: {np.mean(ndcg_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeltaZero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
